#================================================================================================================================================================
#### Configuration du bootstrap du cluster control plane avec kubeadm
## https://github.com/kubernetes-sigs/cluster-api/blob/main/controlplane/kubeadm/config/crd/bases/controlplane.cluster.x-k8s.io_kubeadmcontrolplanetemplates.yaml 
#================================================================================================================================================================

apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: KubeadmControlPlane
metadata:
  name: capg-kubeadm-control-plane
  namespace: capg-management-clusterclass
spec:
  version: v1.34.3
  replicas: 3
  machineTemplate:
    infrastructureRef:
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      kind: GCPMachineTemplate
      name: capg-machine-control-plan
  kubeadmConfigSpec:
    format: cloud-config

    preKubeadmCommands:
      - hostnamectl set-hostname "{{ v1.local_hostname }}"
      - | 
        echo "[preKubeadmCommands] Start /etc/hosts configuration !"
        IPV4=$(hostname -I | awk '{print $1}')
        HOSTNAME="{{ v1.local_hostname }}"
        if ! grep -q "$HOSTNAME" /etc/hosts; then
          echo "$IPV4 $HOSTNAME" >> /etc/hosts
          echo "[preKubeadmCommands] Added $IPV4 $HOSTNAME to /etc/hosts done !"
        else
          echo "[preKubeadmCommands] $HOSTNAME already exist on /etc/hosts"
        fi
        echo "[preKubeadmCommands] End /etc/hosts configuration !"

    clusterConfiguration:
      clusterName: sdxproximaswapmgt
      apiServer:
        extraArgs: 
          audit-log-path: "/var/log/kubernetes/audit/audit.log"
          audit-log-maxage: "30"
          audit-log-maxbackup: "3"
          audit-log-maxsize: "100"
        timeoutForControlPlane: 20m
      controllerManager:
        extraArgs:
          cloud-provider: external
      etcd:
        local:
          dataDir: /var/lib/etcd

    initConfiguration:
      localAPIEndpoint:
        advertiseAddress: "0.0.0.0"
        bindPort: 6443
      nodeRegistration:
        criSocket: unix:///var/run/containerd/containerd.sock
        kubeletExtraArgs:
          cloud-provider: external
          cgroup-driver: systemd
          provider-id: gce://capg-486910/{{ v1.availability_zone }}/{{ v1.local_hostname }}
        name: '{{ v1.local_hostname }}'
      skipPhases:
        - addon/kube-proxy

    joinConfiguration:
      controlPlane:
        localAPIEndpoint:
          advertiseAddress: "0.0.0.0"
          bindPort: 6443
      nodeRegistration:
        criSocket: unix:///var/run/containerd/containerd.sock
        kubeletExtraArgs:
          cloud-provider: external
          provider-id: gce://capg-486910/{{ v1.availability_zone }}/{{ v1.local_hostname }}
        name: '{{ v1.local_hostname }}'

    postKubeadmCommands:
      - |
        HOSTNAME="{{ v1.local_hostname }}"
        echo "[postKubeadmCommands] Wait node added to the cluster"
        until sudo kubectl get node $HOSTNAME --kubeconfig /etc/kubernetes/admin.conf >/dev/null 2>&1; do
          echo "[postKubeadmCommands] wait for node added"
          sleep 5
        done

        echo "[postKubeadmCommands] Install Helm"
        wget https://get.helm.sh/helm-v3.17.0-linux-amd64.tar.gz
        tar -zxvf helm-v3.17.0-linux-amd64.tar.gz
        mv linux-amd64/helm /usr/local/bin/helm
        rm helm-v3.17.0-linux-amd64.tar.gz
        helm version

        echo "[postKubeadmCommand] Install Cilium CNI"
        sudo helm repo add cilium https://helm.cilium.io/
        sudo helm repo update
        sudo helm upgrade --install cilium cilium/cilium --version 1.18.5 --namespace kube-system --set k8sServiceHost=34.102.158.215 --set k8sServicePort=443 --set kubeProxyReplacement=true --set hubble.relay.enabled=true --set ipam.operator.clusterPoolIPv4PodCIDRList={"10.244.0.0/16"} --set ipam.operator.clusterPoolIPv4MaskSize=24 --kubeconfig /etc/kubernetes/admin.conf

        echo "[postKubeadmCommand] Waiting for node $HOSTNAME to become ready"
        until sudo kubectl get node $HOSTNAME --kubeconfig /etc/kubernetes/admin.conf -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}' | grep -q "True"; do
          echo "[postKubeadmiCommands] Node not ready yet !"
          sleep 5
        done

        echo "[postKubeadmCommands] Node is Ready. Removing taint if it exists..."
        sudo kubectl taint nodes $HOSTNAME --kubeconfig /etc/kubernetes/admin.conf node.cloudprovider.kubernetes.io/uninitialized:NoSchedule- || true
        echo "[postKubeadmCommands] Completed taint removal process."